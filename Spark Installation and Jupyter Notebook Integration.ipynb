{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "To set up Spark on your own computer and integrate PySpark with Jupyter Notebook. We can use Spark in two modes:\n",
    "\n",
    "* **Local mode**: the entire Spark application runs on a single machine. You'll use local mode to prototype Spark code on your own computer. (This is the easier setup.)\n",
    "* **Cluster mode**: the Spark application runs across multiple machines. You'll use cluster mode when you want to run your Spark application across multiple machines in a cloud environment like Amazon Web Services, Microsoft Azure, or Digital Ocean.\n",
    "\n",
    "We'll cover the instructions for installing Spark in local mode on Windows, Mac, and Linux. We'll cover how to install Spark in cluster mode in the data engineering track.\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/Apache-Spark/blob/main/Images/spark_components.jpg?raw=true\">\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark runs on the Java Virtual Machine (JVM), which comes in the Java SE Development Kit (JDK). We recommend installing Java SE Development Kit version 7 or higher, which you can download from Oracle’s website:\n",
    "    \n",
    "* http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\n",
    "   \n",
    "Any version after JDK 7 works, so you can download any of the versions on this page. Select the appropriate installation file for your operating system.\n",
    "    \n",
    "If you're on Windows or Linux, choose the correct instruction set architecture (x86 or x64) for your computer. Each computer chip has a specific instruction set architecture that determines the maximum amount of memory it can work with. The two main types are x86 (32 bit) and x64 (64-bit). If you're not sure which one your computer has, you can find out in this guide if you're on Windows or this one if you're on Linux.\n",
    "\n",
    "To verify that the installation worked, launch your command line application (Command Prompt for Windows and Terminal for Mac and Linux) and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java version \"1.7.0_79\"\n",
    "#Java(TM) SE Runtime Environment (build 1.7.0_79-b15)\n",
    "#Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the exact numbers probably won't match, the important thing to verify is that the version is larger than 1.7. This number actually represents Version 7. If you're interested, you can read why at Oracle's website.\n",
    "\n",
    "If running java -version returned an error or a different version than the one you just installed, your Java JDK installation most likely wasn't added to your PATH properly. Read this post to learn more about how to properly add the Java executable to your PATH."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the JVM set up, let's move on to Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you've installed JDK, you could technically download the original source code and build Spark on your computer. Building from the source code is the process of generating an executable program for your machine. It involves [many steps](https://stackoverflow.com/questions/1622506/programming-definitions-what-exactly-is-building/1622520#1622520). While there are some performance benefits to building Spark from source, it takes a while to do, and it's hard to debug if the build fails.\n",
    "\n",
    "We'll download and work with a pre-built version of Spark instead. Navigate to the [Spark downloads page](http://spark.apache.org/downloads.html) and select the following options:\n",
    "\n",
    "1. 1.6.2<br/>\n",
    "Note: Any Spark version prior to 2.0.0 is incompatible with Python 3.6. If you have Python 3.6, we recommend downloading one of the newer versions of Spark.\n",
    "2. Pre-built for Hadoop 2.6\n",
    "3. Direct Download\n",
    "\n",
    "Next, click the link that appears in Step 4 to download Spark as a .TGZ file to your computer. Open your command line application and navigate to the folder where you downloaded it. Unzip the file and move the resulting folder into your home directory. Windows doesn't have a built-in utility that can unzip tgz files — we recommend downloading and using [7-Zip](https://www.7-zip.org/). Once you've unzipped the file, move the resulting folder into your home directory.\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/Apache-Spark/blob/main/Images/cmd_bin_pyspark.JPG?raw=true\">\n",
    " \n",
    "While this results in a lot of output, you can see that the shell automatically initialized the SparkContext object and assigned it to the variable sc.\n",
    "\n",
    "You don't have to run bin/pyspark from the folder that contains it. Because it's in your home directory, you can use \"~/spark-1.6.1-bin-hadoop2.6/bin/pyspark\" to launch the PySpark shell from other directories on your machine (Note: replace 1.6.1 with 1.6.2 for newer version users). This way, you can switch to the directory that contains the data you want to use, launch the PySpark shell, and read the data in without using its full path. The folder you're in when you launch the PySpark shell will be the local context for working with files in Spark.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting with Jupyter Notebook\n",
    "\n",
    "You can make your Jupyter Notebook application aware of Spark in a few different ways:\n",
    "* One is to create a configuration file and launch Jupyter Notebook with that configuration\n",
    "* Another is to import PySpark at runtime\n",
    "\n",
    "We'll focus on the latter approach so you won't have to restart Jupyter Notebook each time you want to use Spark\n",
    "\n",
    "First, you'll need to copy the full path to the pre-built Spark folder and set it as a shell environment variable. This way, you can specify Spark's location a single time, and every Python program you write will have access to it. If you move the Spark folder, you can change the path specification once and your code will work fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mac / Linux\n",
    "\n",
    "* Use nano or another text editor to open your shell environment's configuration file. If you're using the default Terminal application, the file should be in ~/.bash_profile . If you're using ZSH instead, your configuration file will be in ~/.zshrc.\n",
    "\n",
    "* Add the following line to the end of the file, replacing {full path to Spark} with the actual path to Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export SPARK_HOME=\"{full path to Spark, eg /users/home/jeff/spark-2.0.1-bin-hadoop2.7/}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exit the text editor and run either source ~/.bash_profile or source ~/.zshrc so the shell reads in and applies the update you made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows\n",
    "\n",
    "* If you've never added environment variables, read this tutorial before you proceed.\n",
    "* Set the SPARK_HOME environment variable to the full path of the Spark folder (e.g., c:/Users/Rohan/spark-2.0.1-bin-hadoop2.7/).\n",
    "\n",
    "Next, let's install the findspark Python library, which looks up the location of PySpark using the environment variable we just set. Use pip to install the findspark library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-1.4.2-py2.py3-none-any.whl (4.2 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('recent-grads.csv', <http.client.HTTPMessage at 0x2f28132f408>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlretrieve(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/college-majors/recent-grads.csv\", \"recent-grads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find path to Spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Rank,Major_code,Major,Total,Men,Women,Major_category,ShareWomen,Sample_size,Employed,Full_time,Part_time,Full_time_year_round,Unemployed,Unemployment_rate,Median,P25th,P75th,College_jobs,Non_college_jobs,Low_wage_jobs'],\n",
       " ['1,2419,PETROLEUM ENGINEERING,2339,2057,282,Engineering,0.120564344,36,1976,1849,270,1207,37,0.018380527,110000,95000,125000,1534,364,193'],\n",
       " ['2,2416,MINING AND MINERAL ENGINEERING,756,679,77,Engineering,0.101851852,7,640,556,170,388,85,0.117241379,75000,55000,90000,350,257,50'],\n",
       " ['3,2415,METALLURGICAL ENGINEERING,856,725,131,Engineering,0.153037383,3,648,558,133,340,16,0.024096386,73000,50000,105000,456,176,0'],\n",
       " ['4,2417,NAVAL ARCHITECTURE AND MARINE ENGINEERING,1258,1123,135,Engineering,0.107313196,16,758,1069,150,692,40,0.050125313,70000,43000,80000,529,102,0'],\n",
       " ['5,2405,CHEMICAL ENGINEERING,32260,21239,11021,Engineering,0.341630502,289,25694,23170,5180,16697,1672,0.061097712,65000,50000,75000,18314,4440,972'],\n",
       " ['6,2418,NUCLEAR ENGINEERING,2573,2200,373,Engineering,0.144966965,17,1857,2038,264,1449,400,0.177226407,65000,50000,102000,1142,657,244'],\n",
       " ['7,6202,ACTUARIAL SCIENCE,3777,2110,1667,Business,0.441355573,51,2912,2924,296,2482,308,0.095652174,62000,53000,72000,1768,314,259'],\n",
       " ['8,5001,ASTRONOMY AND ASTROPHYSICS,1792,832,960,Physical Sciences,0.535714286,10,1526,1085,553,827,33,0.021167415,62000,31500,109000,972,500,220'],\n",
       " ['9,2414,MECHANICAL ENGINEERING,91227,80320,10907,Engineering,0.119558903,1029,76442,71298,13101,54639,4650,0.057342278,60000,48000,70000,52844,16384,3253']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import PySpark and initialize SparkContext object\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# Read 'recent-grads.csv' in to an RDD\n",
    "f = sc.textFile('recent-grads.csv')\n",
    "data = f.map(lambda line: line.split('\\n'))\n",
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
