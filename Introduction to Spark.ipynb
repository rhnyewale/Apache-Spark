{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "**Big data** has received a lot of attention over the last few years, and for good reason. Companies like Google and Yahoo! have grown their user bases significantly, and they're collecting more information on how people interact with their products.\n",
    "\n",
    "While software companies have gotten better at collecting massive amounts of data, their ability to analyze and make sense of it hasn't always improved. Because existing technologies couldn't analyze such large quantities of data as the data collection trends increased, companies like Google, Facebook, Yahoo!, and LinkedIn had to build new tools and approaches that could do the job.\n",
    "\n",
    "Engineers initially tried using bigger and more powerful computers to process the data, but they still ran into limits for many computational problems. Along the way, they developed paradigms like **MapReduce** that efficiently distribute calculations over hundreds or thousands of computers to calculate the result. Hadoop is an open source project that quickly became the dominant processing toolkit for big data.\n",
    "\n",
    "# Hadoop\n",
    "\n",
    "Hadoop consists of a file system (Hadoop Distributed File System, or HDFS) and its own implementation of the MapReduce paradigm. MapReduce converts computations into Map and Reduce steps that Hadoop can easily distribute over many machines. We'll cover how MapReduce works later in this lesson.\n",
    "\n",
    "Hadoop made it possible to analyze large data sets, but it relied heavily on disk storage (rather than memory) for computation. While it's inexpensive to store large amounts of data this way, it makes accessing and processing it much slower.\n",
    "\n",
    "Hadoop wasn't a great solution for calculations requiring multiple passes over the same data or many intermediate steps, due to the need to write to and read from the disk between each step. This drawback also made Hadoop difficult to use for interactive data analysis, the main task for data scientists.\n",
    "\n",
    "Hadoop also suffered from suboptimal support for the additional libraries many data scientists needed, such as SQL and machine learning implementations. Once the cost of RAM (computer memory) started to drop significantly, augmenting or replacing Hadoop by storing data in-memory quickly emerged as an appealing alternative.\n",
    "\n",
    "The UC Berkeley AMP Lab spearheaded groundbreaking work to develop Spark, which uses distributed, in-memory data structures to accelerate data processing workloads by several orders of magnitude. If you're interested in learning more, you can check out some of the original papers on the Apache Spark homepage.\n",
    "\n",
    "https://spark.apache.org/research.html\n",
    "\n",
    "\n",
    "## Data Structure in Spark\n",
    "\n",
    "The core data structure in Spark is a **resilient distributed data set (RDD)**. As the name suggests, an RDD is Spark's representation of a data set that's distributed across the RAM, or memory, of a cluster of many machines. An RDD object is essentially a collection of elements we can use to hold lists of tuples, dictionaries, lists, etc. Similar to a pandas DataFrame, we can load a data set into an RDD, and then run any of the methods accessible to that object.\n",
    "\n",
    "## PySpark\n",
    "\n",
    "While the Spark toolkit is in Scala, a language that compiles down to bytecode for the JVM, the open source community has developed a wonderful toolkit called **PySpark** that allows us to **interface with RDDs in Python**. Thanks to a library called **Py4J**, Python can interface with Java objects (in our case RDDs). **Py4J** is also one of the tools that makes PySpark work.\n",
    "\n",
    "## Data set\n",
    "\n",
    "We'll work with a data set containing the names of all of the guests who have appeared on The Daily Show.\n",
    "\n",
    "To begin, we'll load the data set into an RDD. We're using the TSV version of FiveThirtyEight's data set. TSV files use a tab character (\"\\t\") as the delimiter, instead of the comma (\",\") that CSV files use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark, the **SparkContext object** manages the connection to the clusters, and it coordinates processes on those clusters. Specifically, it connects to the cluster managers. The cluster managers control the executors that run the computations. Here's a diagram from the Spark documentation that will help you visualize the architecture:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
